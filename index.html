<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Can LLMs Correct Themselves? - Paper Introduction</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            line-height: 1.6;
        }

        h1,
        h2 {
            color: #333;
        }

        img {
            width: 80%;
            margin: 20px 0;
        }

        .container {
            max-width: 800px;
            margin: auto;
        }

        .section {
            margin-bottom: 40px;
        }
    </style>
</head>

<body>
    <div class="container">
        <h1>Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs</h1>
        <p><strong>Abstract:</strong> This paper introduces CorrectBench, a systematic benchmark to evaluate
            self-correction capabilities in large language models (LLMs), covering commonsense reasoning, mathematical
            reasoning, and code generation. The study reveals self-correction strategies can improve reasoning accuracy
            but may impact efficiency.</p>

        <div class="section">
            <h2>Introduction to Self-Correction</h2>
            <p>Large Language Models (LLMs) have greatly advanced, but ensuring their reliability and accuracy remains
                challenging. Self-correction methods help refine LLM responses through iterative feedback, enhancing
                performance particularly in complex reasoning tasks.</p>
<!--             <img src="static/images/poster.png" alt="MY ALT TEXT"/> -->
            <img src="fig/Mixture.png"
                alt="Self-Correction Concept Illustration">
        </div>

        <div class="section">
            <h2>What is CorrectBench?</h2>
            <p>CorrectBench is a detailed benchmark developed to measure the effectiveness of self-correction methods in
                LLMs across various reasoning tasks. It evaluates intrinsic, external, and fine-tuned correction methods
                systematically.</p>
            <img src="fig/overview.png" alt="CorrectBench Framework">
        </div>

        <div class="section">
            <h2>Self-Correction Types</h2>
            <ul>
                <li><strong>Intrinsic Correction:</strong> Internal error correction without external aids.</li>
                <li><strong>External Correction:</strong> Uses external tools and resources to correct errors.</li>
                <li><strong>Fine-tuned Correction:</strong> Model adjustments through targeted fine-tuning.</li>
            </ul>
        </div>

        <div class="section">
            <h2>Evaluation and Results</h2>
            <p>Findings indicate substantial accuracy improvements with self-correction, especially in mathematical and
                complex reasoning tasks. However, mixing methods may increase computational overhead significantly.</p>
            <img src="fig/increase_intrinsic.png" alt="Evaluation Results">
        </div>

        <div class="section">
            <h2>Conclusion and Implications</h2>
            <p>The benchmark emphasizes self-correction as a valuable tool for enhancing LLM performance, with a clear
                need to balance reasoning capabilities and efficiency. Future research should optimize this trade-off
                for practical applications.</p>
        </div>
    </div>
</body>

</html>
